{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF version\n",
    "\n",
    "This notebook are design for TF-IDF version. For experiment and performanance consideration, original dataset are resampled into a smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\derekhsu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('names')\n",
    "\n",
    "\n",
    "# Tokeenization\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Vectorizer\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Utilities\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import operator \n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Word Embedding\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Spell check\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "from gensim.matutils import unitvec\n",
    "\n",
    "import logging\n",
    "import normalise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(\"Progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and resampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(\"../input/train.csv\")\n",
    "testing_dataset = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_tiny = training_dataset.sample(frac=0.01, random_state=42)\n",
    "training_dataset_small = training_dataset.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "#stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "#stopwords_combined = set.union(set(stopwords_json['en']), stopwords_en_withpunct)\n",
    "stopwords = stopwords_en_withpunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(text_to_word_sequence(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    tokens =  [word for word in lemmatize_sent(text) \n",
    "            if word not in stopwords\n",
    "            and not word.isdigit()]\n",
    "    tokens = normalise.normalise(text=tokens, user_abbrevs=custom_dictionary, verbose=False)\n",
    "   \n",
    "    \n",
    "    return tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        stop_words = self.get_stop_words()\n",
    "    \n",
    "        def analyser(doc):\n",
    "            if (self.lowercase == True):\n",
    "                doc = doc.lower()\n",
    "            tokens = preprocess_text(doc)\n",
    "            \n",
    "            return(self._word_ngrams(tokens, stop_words))\n",
    "        return (analyser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_TFIDF = 1000\n",
    "ngram_range_TFIDF = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = CustomVectorizer(stop_words=stopwords,\n",
    "                                    ngram_range=ngram_range_TFIDF,\n",
    "                                    max_features=max_features_TFIDF,\n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='strict',\n",
    "                                    strip_accents = None,\n",
    "                                    lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors_tiny = tfidf_vectorizer.fit_transform(training_dataset_tiny['question_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review TF-IDF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "sums = tfidf_vectors_tiny.sum(axis=0)\n",
    "\n",
    "word_info = []\n",
    "for col, term in enumerate(terms):\n",
    "    word_info.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(word_info, columns=['term','rank'])\n",
    "keywords = ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>get</td>\n",
       "      <td>250.198821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>best</td>\n",
       "      <td>228.659602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s</td>\n",
       "      <td>213.781729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>use</td>\n",
       "      <td>195.286116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>would</td>\n",
       "      <td>194.455274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>people</td>\n",
       "      <td>190.666262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>good</td>\n",
       "      <td>189.814823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>like</td>\n",
       "      <td>181.068262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>make</td>\n",
       "      <td>176.219783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>india</td>\n",
       "      <td>133.550014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>n't</td>\n",
       "      <td>126.535688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>one</td>\n",
       "      <td>120.391107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>know</td>\n",
       "      <td>117.719265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>think</td>\n",
       "      <td>115.835123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>``</td>\n",
       "      <td>112.093154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>''</td>\n",
       "      <td>111.236061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>work</td>\n",
       "      <td>106.461540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>many</td>\n",
       "      <td>105.580571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>way</td>\n",
       "      <td>104.712710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>go</td>\n",
       "      <td>101.342756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term        rank\n",
       "390     get  250.198821\n",
       "117    best  228.659602\n",
       "3        's  213.781729\n",
       "946     use  195.286116\n",
       "986   would  194.455274\n",
       "651  people  190.666262\n",
       "397    good  189.814823\n",
       "519    like  181.068262\n",
       "539    make  176.219783\n",
       "454   india  133.550014\n",
       "593     n't  126.535688\n",
       "627     one  120.391107\n",
       "494    know  117.719265\n",
       "904   think  115.835123\n",
       "24       ``  112.093154\n",
       "0        ''  111.236061\n",
       "983    work  106.461540\n",
       "544    many  105.580571\n",
       "963     way  104.712710\n",
       "395      go  101.342756"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "Due to the result of TF-IDF vectorizer contains some meaningless terms such as \"'s'\", \"n't\" and part of them can be restored to their original form, we need to preprocess the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dictionary = { \"won't\": 'will not',  \n",
    "                      \"what's\": 'what is'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.union(set(['']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    \n",
    "    tokens =  [word for word in lemmatize_sent(text) \n",
    "            if word not in stopwords\n",
    "            and not word.isdigit()]\n",
    "    try: \n",
    "        tokens = normalise.normalise(text=tokens, user_abbrevs=custom_dictionary, verbose=False)\n",
    "    except:\n",
    "        result = []\n",
    "        for text in tokens:\n",
    "            try:\n",
    "                result.append(normalise.normalise(texts, verbose=False))\n",
    "            except:\n",
    "                result.append(text)\n",
    "        tokens = result\n",
    "    tokens = [word for word in tokens\n",
    "          if word not in stopwords]\n",
    "    return tokens;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TF-IDF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        stop_words = self.get_stop_words()\n",
    "    \n",
    "        def analyser(doc):\n",
    "            if (self.lowercase == True):\n",
    "                doc = doc.lower()\n",
    "            tokens = preprocess_text(doc)\n",
    "            \n",
    "            return(self._word_ngrams(tokens, stop_words))\n",
    "        return (analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_preprocessing = CustomVectorizer(stop_words=stopwords,\n",
    "                                    ngram_range=ngram_range_TFIDF,\n",
    "                                    max_features=max_features_TFIDF,\n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='strict',\n",
    "                                    strip_accents = None,\n",
    "                                    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors_tiny_preprocessing = tfidf_vectorizer_preprocessing.fit_transform(training_dataset_tiny['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer_preprocessing.get_feature_names()\n",
    "sums = tfidf_vectors_tiny_preprocessing.sum(axis=0)\n",
    "\n",
    "word_info = []\n",
    "for col, term in enumerate(terms):\n",
    "    word_info.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(word_info, columns=['term','rank'])\n",
    "keywords = ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>get</td>\n",
       "      <td>260.012017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>best</td>\n",
       "      <td>225.979241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>use</td>\n",
       "      <td>203.734527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>would</td>\n",
       "      <td>193.232869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>people</td>\n",
       "      <td>187.652073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>good</td>\n",
       "      <td>187.447267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>like</td>\n",
       "      <td>184.340280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>make</td>\n",
       "      <td>177.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>india</td>\n",
       "      <td>131.842838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>know</td>\n",
       "      <td>119.230941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>one</td>\n",
       "      <td>117.604816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>think</td>\n",
       "      <td>114.010179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>many</td>\n",
       "      <td>107.846336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>work</td>\n",
       "      <td>105.739679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>way</td>\n",
       "      <td>105.351229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>go</td>\n",
       "      <td>104.870071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>time</td>\n",
       "      <td>103.550771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>take</td>\n",
       "      <td>99.578697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>find</td>\n",
       "      <td>97.035877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>year</td>\n",
       "      <td>95.225359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ever</td>\n",
       "      <td>94.364982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>happen</td>\n",
       "      <td>91.393769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>life</td>\n",
       "      <td>90.746104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>difference</td>\n",
       "      <td>86.687175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>much</td>\n",
       "      <td>86.352260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>job</td>\n",
       "      <td>86.140752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>feel</td>\n",
       "      <td>85.982765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>become</td>\n",
       "      <td>84.742362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>book</td>\n",
       "      <td>81.920703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>someone</td>\n",
       "      <td>80.701933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>relation</td>\n",
       "      <td>8.711223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>destroy</td>\n",
       "      <td>8.710787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>reality</td>\n",
       "      <td>8.707548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>among</td>\n",
       "      <td>8.697073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>finance</td>\n",
       "      <td>8.666345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>ship</td>\n",
       "      <td>8.654838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>four</td>\n",
       "      <td>8.649651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>focus</td>\n",
       "      <td>8.625185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>dark</td>\n",
       "      <td>8.618513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>pop</td>\n",
       "      <td>8.618370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>category</td>\n",
       "      <td>8.601145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>attention</td>\n",
       "      <td>8.575157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>ignore</td>\n",
       "      <td>8.509335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>i've</td>\n",
       "      <td>8.457960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>except</td>\n",
       "      <td>8.444094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>trump's</td>\n",
       "      <td>8.424601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>century</td>\n",
       "      <td>8.418133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>background</td>\n",
       "      <td>8.392072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>wing</td>\n",
       "      <td>8.373547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>direct</td>\n",
       "      <td>8.280534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>left</td>\n",
       "      <td>8.271959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>throw</td>\n",
       "      <td>8.228891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>financial</td>\n",
       "      <td>8.153959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>racist</td>\n",
       "      <td>8.059867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>murder</td>\n",
       "      <td>7.934817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>eu</td>\n",
       "      <td>7.738703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>almost</td>\n",
       "      <td>7.515389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>currently</td>\n",
       "      <td>7.224283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>tend</td>\n",
       "      <td>7.183918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>despite</td>\n",
       "      <td>6.912248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           term        rank\n",
       "384         get  260.012017\n",
       "106        best  225.979241\n",
       "947         use  203.734527\n",
       "990       would  193.232869\n",
       "653      people  187.652073\n",
       "392        good  187.447267\n",
       "518        like  184.340280\n",
       "539        make  177.489796\n",
       "454       india  131.842838\n",
       "491        know  119.230941\n",
       "629         one  117.604816\n",
       "907       think  114.010179\n",
       "544        many  107.846336\n",
       "987        work  105.739679\n",
       "964         way  105.351229\n",
       "390          go  104.870071\n",
       "914        time  103.550771\n",
       "888        take   99.578697\n",
       "349        find   97.035877\n",
       "994        year   95.225359\n",
       "305        ever   94.364982\n",
       "411      happen   91.393769\n",
       "516        life   90.746104\n",
       "248  difference   86.687175\n",
       "588        much   86.352260\n",
       "482         job   86.140752\n",
       "340        feel   85.982765\n",
       "99       become   84.742362\n",
       "115        book   81.920703\n",
       "836     someone   80.701933\n",
       "..          ...         ...\n",
       "746    relation    8.711223\n",
       "239     destroy    8.710787\n",
       "734     reality    8.707548\n",
       "46        among    8.697073\n",
       "347     finance    8.666345\n",
       "809        ship    8.654838\n",
       "362        four    8.649651\n",
       "354       focus    8.625185\n",
       "222        dark    8.618513\n",
       "678         pop    8.618370\n",
       "147    category    8.601145\n",
       "78    attention    8.575157\n",
       "443      ignore    8.509335\n",
       "440        i've    8.457960\n",
       "314      except    8.444094\n",
       "933     trump's    8.424601\n",
       "151     century    8.418133\n",
       "87   background    8.392072\n",
       "980        wing    8.373547\n",
       "252      direct    8.280534\n",
       "508        left    8.271959\n",
       "913       throw    8.228891\n",
       "348   financial    8.153959\n",
       "725      racist    8.059867\n",
       "591      murder    7.934817\n",
       "300          eu    7.738703\n",
       "38       almost    7.515389\n",
       "217   currently    7.224283\n",
       "900        tend    7.183918\n",
       "238     despite    6.912248\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "tfidf_vectors_tiny_preprocessing_svd = svd.fit_transform(X=tfidf_vectors_tiny_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_vectors_tiny_preprocessing_svd = pd.DataFrame(tfidf_vectors_tiny_preprocessing_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf_vectors_tiny_preprocessing_svd['target'] = training_dataset_tiny['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.071309\n",
       "1        0.042300\n",
       "2        0.154622\n",
       "3        0.035460\n",
       "4        0.078361\n",
       "5        0.014485\n",
       "6        0.006350\n",
       "7        0.187351\n",
       "8        0.039864\n",
       "9        0.048866\n",
       "10       0.036474\n",
       "11       0.007866\n",
       "12       0.162376\n",
       "13       0.063860\n",
       "14       0.000000\n",
       "15       0.046308\n",
       "16       0.095813\n",
       "17       0.012549\n",
       "18       0.031626\n",
       "19       0.012405\n",
       "20       0.055805\n",
       "21       0.114282\n",
       "22       0.123356\n",
       "23       0.047654\n",
       "24       0.000000\n",
       "25       0.034628\n",
       "26       0.000000\n",
       "27       0.034757\n",
       "28       0.165780\n",
       "29       0.277131\n",
       "           ...   \n",
       "13027    0.115830\n",
       "13028    0.041753\n",
       "13029    0.023152\n",
       "13030    0.083407\n",
       "13031    0.032427\n",
       "13032    0.172420\n",
       "13033    0.140473\n",
       "13034    0.040145\n",
       "13035    0.024546\n",
       "13036    0.018229\n",
       "13037    0.118853\n",
       "13038    0.014492\n",
       "13039    0.019053\n",
       "13040    0.025547\n",
       "13041    0.121225\n",
       "13043    0.117825\n",
       "13044    0.106875\n",
       "13045    0.326132\n",
       "13046    0.081154\n",
       "13047    0.036418\n",
       "13048    0.125254\n",
       "13049    0.031861\n",
       "13050    0.077755\n",
       "13052    0.190872\n",
       "13053    0.032394\n",
       "13055    0.000000\n",
       "13056    0.007355\n",
       "13057    0.043879\n",
       "13059    0.051478\n",
       "13060    0.029917\n",
       "Name: 0, Length: 12219, dtype: float64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_vectors_tiny_preprocessing_svd[df_tfidf_vectors_tiny_preprocessing_svd['target'] == 0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=df_tfidf_vectors_tiny_preprocessing_svd[df_tfidf_vectors_tiny_preprocessing_svd['target'] == 0][0], y=df_tfidf_vectors_tiny_preprocessing_svd[df_tfidf_vectors_tiny_preprocessing_svd['target'] == 0][1])\n",
    "plt.scatter(x=df_tfidf_vectors_tiny_preprocessing_svd[df_tfidf_vectors_tiny_preprocessing_svd['target'] == 1][0], y=df_tfidf_vectors_tiny_preprocessing_svd[df_tfidf_vectors_tiny_preprocessing_svd['target'] == 1][1])\n",
    "plt.legend(['Sicenere', 'Insicenere'])\n",
    "plt.title(\"Data Visualization by SVD to 1% of dataset\")\n",
    "plt.savefig(\"Data_Visualization_by_SVD_to_1%_of_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small, test_small = train_test_split(training_dataset_small, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_vectors = tfidf_vectorizer_preprocessing.fit_transform(train_small['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer_preprocessing.get_feature_names()\n",
    "sums = train_small_vectors.sum(axis=0)\n",
    "\n",
    "word_info = []\n",
    "for col, term in enumerate(terms):\n",
    "    word_info.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(word_info, columns=['term','rank'])\n",
    "keywords = ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>get</td>\n",
       "      <td>2083.868798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>best</td>\n",
       "      <td>1948.348078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>people</td>\n",
       "      <td>1547.257325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>good</td>\n",
       "      <td>1506.621599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>would</td>\n",
       "      <td>1503.729631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>use</td>\n",
       "      <td>1455.662114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>make</td>\n",
       "      <td>1449.159249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>like</td>\n",
       "      <td>1441.696606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>india</td>\n",
       "      <td>1038.279183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>one</td>\n",
       "      <td>977.477287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>think</td>\n",
       "      <td>965.995284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>know</td>\n",
       "      <td>897.680497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>way</td>\n",
       "      <td>888.882305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>work</td>\n",
       "      <td>886.101701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>take</td>\n",
       "      <td>850.398692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>many</td>\n",
       "      <td>815.353607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>time</td>\n",
       "      <td>802.704653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>find</td>\n",
       "      <td>782.989511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>go</td>\n",
       "      <td>777.331128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>become</td>\n",
       "      <td>771.176194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term         rank\n",
       "377     get  2083.868798\n",
       "90     best  1948.348078\n",
       "658  people  1547.257325\n",
       "385    good  1506.621599\n",
       "990   would  1503.729631\n",
       "946     use  1455.662114\n",
       "534    make  1449.159249\n",
       "512    like  1441.696606\n",
       "444   india  1038.279183\n",
       "634     one   977.477287\n",
       "907   think   965.995284\n",
       "485    know   897.680497\n",
       "964     way   888.882305\n",
       "987    work   886.101701\n",
       "889    take   850.398692\n",
       "540    many   815.353607\n",
       "911    time   802.704653\n",
       "344    find   782.989511\n",
       "383      go   777.331128\n",
       "86   become   771.176194"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small_vectors = tfidf_vectorizer_preprocessing.transform(test_small['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_small_vectors.toarray(), train_small['target'])\n",
    "y_pred = gnb.predict(test_small_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.534510\n",
      "Recall score: 0.898196\n",
      "F1 score: 0.186513\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score: %f\"%accuracy_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"Recall score: %f\"%recall_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"F1 score: %f\"%f1_score(y_true=test_small['target'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (2 gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_preprocessing_2gram = CustomVectorizer(stop_words=stopwords,\n",
    "                                    ngram_range=(2, 2),\n",
    "                                    max_features=max_features_TFIDF,\n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='strict',\n",
    "                                    strip_accents = None,\n",
    "                                    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_vectors_2gram = tfidf_vectorizer_preprocessing_2gram.fit_transform(train_small['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small_vectors_2gram = tfidf_vectorizer_preprocessing_2gram.transform(test_small['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer_preprocessing_2gram.get_feature_names()\n",
    "sums = train_small_vectors_2gram.sum(axis=0)\n",
    "\n",
    "word_info = []\n",
    "for col, term in enumerate(terms):\n",
    "    word_info.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(word_info, columns=['term','rank'])\n",
    "keywords = ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>best way</td>\n",
       "      <td>506.941715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>year old</td>\n",
       "      <td>367.787437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>would happen</td>\n",
       "      <td>311.478065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>united state</td>\n",
       "      <td>259.358930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>look like</td>\n",
       "      <td>231.775195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>donald trump</td>\n",
       "      <td>204.689129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>feel like</td>\n",
       "      <td>179.263745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>get rid</td>\n",
       "      <td>178.903574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>high school</td>\n",
       "      <td>172.610831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>get job</td>\n",
       "      <td>163.122182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>computer science</td>\n",
       "      <td>160.251212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>even though</td>\n",
       "      <td>159.343310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>social medium</td>\n",
       "      <td>158.247507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>would win</td>\n",
       "      <td>156.763726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>many people</td>\n",
       "      <td>153.107185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>long take</td>\n",
       "      <td>148.370436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>best book</td>\n",
       "      <td>146.303643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>what's best</td>\n",
       "      <td>135.785592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>best place</td>\n",
       "      <td>134.777555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>would like</td>\n",
       "      <td>128.733953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 term        rank\n",
       "84           best way  506.941715\n",
       "992          year old  367.787437\n",
       "963      would happen  311.478065\n",
       "877      united state  259.358930\n",
       "469         look like  231.775195\n",
       "190      donald trump  204.689129\n",
       "246         feel like  179.263745\n",
       "308           get rid  178.903574\n",
       "376       high school  172.610831\n",
       "297           get job  163.122182\n",
       "153  computer science  160.251212\n",
       "218       even though  159.343310\n",
       "746     social medium  158.247507\n",
       "982         would win  156.763726\n",
       "510       many people  153.107185\n",
       "463         long take  148.370436\n",
       "61          best book  146.303643\n",
       "922       what's best  135.785592\n",
       "76         best place  134.777555\n",
       "964        would like  128.733953"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_small_vectors_2gram.toarray(), train_small['target'])\n",
    "y_pred = gnb.predict(test_small_vectors_2gram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.169391\n",
      "Recall score: 0.969072\n",
      "F1 score: 0.121752\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score: %f\"%accuracy_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"Recall score: %f\"%recall_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"F1 score: %f\"%f1_score(y_true=test_small['target'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (1 and 2 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_preprocessing_12gram = CustomVectorizer(stop_words=stopwords,\n",
    "                                    ngram_range=(1, 2),\n",
    "                                    max_features=max_features_TFIDF,\n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='strict',\n",
    "                                    strip_accents = None,\n",
    "                                    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_vectors_12gram = tfidf_vectorizer_preprocessing_12gram.fit_transform(train_small['question_text'])\n",
    "test_small_vectors_12gram = tfidf_vectorizer_preprocessing_12gram.transform(test_small['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer_preprocessing_12gram.get_feature_names()\n",
    "sums = train_small_vectors_12gram.sum(axis=0)\n",
    "\n",
    "word_info = []\n",
    "for col, term in enumerate(terms):\n",
    "    word_info.append( (term, sums[0,col] ))\n",
    "\n",
    "ranking = pd.DataFrame(word_info, columns=['term','rank'])\n",
    "keywords = ranking.sort_values('rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>get</td>\n",
       "      <td>2053.384556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>best</td>\n",
       "      <td>1889.596268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>people</td>\n",
       "      <td>1531.067002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>good</td>\n",
       "      <td>1508.959818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>would</td>\n",
       "      <td>1467.945171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>use</td>\n",
       "      <td>1459.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>make</td>\n",
       "      <td>1451.518237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>like</td>\n",
       "      <td>1410.105457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>india</td>\n",
       "      <td>1037.963893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>one</td>\n",
       "      <td>977.714246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>think</td>\n",
       "      <td>958.857992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>know</td>\n",
       "      <td>897.036128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>work</td>\n",
       "      <td>886.400964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>take</td>\n",
       "      <td>840.583555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>way</td>\n",
       "      <td>838.620883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>many</td>\n",
       "      <td>806.041438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>time</td>\n",
       "      <td>804.631187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>find</td>\n",
       "      <td>783.323652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>go</td>\n",
       "      <td>776.754880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>become</td>\n",
       "      <td>771.907734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term         rank\n",
       "375     get  2053.384556\n",
       "89     best  1889.596268\n",
       "659  people  1531.067002\n",
       "385    good  1508.959818\n",
       "986   would  1467.945171\n",
       "940     use  1459.891000\n",
       "534    make  1451.518237\n",
       "511    like  1410.105457\n",
       "444   india  1037.963893\n",
       "635     one   977.714246\n",
       "903   think   958.857992\n",
       "486    know   897.036128\n",
       "983    work   886.400964\n",
       "885    take   840.583555\n",
       "958     way   838.620883\n",
       "540    many   806.041438\n",
       "907    time   804.631187\n",
       "343    find   783.323652\n",
       "383      go   776.754880\n",
       "85   become   771.907734"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_small_vectors_12gram.toarray(), train_small['target'])\n",
    "y_pred = gnb.predict(test_small_vectors_12gram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.527351\n",
      "Recall score: 0.900129\n",
      "F1 score: 0.184532\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score: %f\"%accuracy_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"Recall score: %f\"%recall_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"F1 score: %f\"%f1_score(y_true=test_small['target'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=30, \n",
    "                       max_depth=120,\n",
    "                       min_samples_leaf=2,\n",
    "                       max_features=\"auto\",\n",
    "                       n_jobs=-1)\n",
    "\n",
    "randomForestClassifier.fit(train_small_vectors, train_small['target'])\n",
    "y_pred = randomForestClassifier.predict(test_small_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.944225\n",
      "Recall score: 0.152706\n",
      "F1 score: 0.245469\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score: %f\"%accuracy_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"Recall score: %f\"%recall_score(y_true=test_small['target'], y_pred=y_pred))\n",
    "print(\"F1 score: %f\"%f1_score(y_true=test_small['target'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you say about feminism?\n",
      "What can you say about feminism\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What', 'can', 'you', 'say', 'about', 'feminism']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = training_dataset.iloc[10]['question_text']\n",
    "print(text)\n",
    "print(clean_text(text))\n",
    "normalise.normalise(clean_text(text), tokenizer=word_tokenize, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessing', 'is', 'the', 'most', 'important', 'step']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise.normalise(clean_text(\"Preprocessing is the most important step\"), tokenizer=text_to_word_sequence, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How did Quebec nationalists see their province as a nation in the 1960s?/0\n",
      "['quebec', 'nationalist', 'see', 'province', 'nation', 'nineteen sixties']\n",
      "Do you have an adopted dog, how would you encourage people to adopt and not shop?/0\n",
      "['adopt', 'dog', 'would', 'encourage', 'people', 'adopt', 'shop']\n",
      "Why does velocity affect time? Does velocity affect space geometry?/0\n",
      "['velocity', 'affect', 'time', 'velocity', 'affect', 'space', 'geometry']\n",
      "How did Otto von Guericke used the Magdeburg hemispheres?/0\n",
      "['otto', 'von', 'guericke', 'use', 'magdeburg', 'hemisphere']\n",
      "Can I convert montra helicon D to a mountain bike by just changing the tyres?/0\n",
      "['convert', 'months', 'helicon', 'mountain', 'bike', 'change', 'tyre']\n",
      "Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?/0\n",
      "['gaze', 'slowly', 'become', 'auschwitz', 'dachau', 'treblinka', 'palestinian']\n",
      "Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?/0\n",
      "['quota', 'automatically', 'ban', 'conservative', 'opinion', 'report', 'liberal', 'view']\n",
      "Is it crazy if I wash or wipe my groceries off? Germs are everywhere./0\n",
      "['crazy', 'wash', 'wipe', 'grocery', 'germ', 'everywhere']\n",
      "Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?/0\n",
      "['thing', 'dress', 'moderately', 'different', 'dress', 'modestly']\n",
      "Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings/lives so you get to have something go your way and feel temporarily at ease. How did things change?/0\n",
      "['ever', 'phase', 'wherein', 'become', 'ignorant', 'people', 'love', 'completely', 'disregard', 'feeling', 'live', 'get', 'something', 'go', 'way', 'feel', 'temporarily', 'ease', 'thing', 'change']\n",
      "What can you say about feminism?/0\n",
      "['say', 'feminism']\n",
      "How were the Calgary Flames founded?/0\n",
      "['salary', 'flame', 'found']\n",
      "What is the dumbest, yet possibly true explanation for Trump being elected?/0\n",
      "['dumb', 'yet', 'possibly', 'true', 'explanation', 'trump', 'elect']\n",
      "Can we use our external hard disk as a OS as well as for data storage.will the data be affected?/0\n",
      "['use', 'external', 'hard', 'disk', 'well', 'data', 'storage', 'data', 'affect']\n",
      "I am 30, living at home and have no boyfriend. I would love a boyfriend and my own home. How can I progress my situation?/0\n",
      "['living', 'home', 'boyfriend', 'would', 'love', 'boyfriend', 'home', 'progress', 'situation']\n",
      "What do you know about Bram Fischer and the Rivonia Trial?/0\n",
      "['know', 'bram', 'pitcher', 'rivonia', 'trial']\n",
      "How difficult is it to find a good instructor to take a class near you?/0\n",
      "['difficult', 'find', 'good', 'instructor', 'take', 'class', 'near']\n",
      "Have you licked the skin of a corpse?/0\n",
      "['lick', 'skin', 'corpse']\n",
      "Do you think Amazon will adopt an in house approach to manufacturing similar to the Tesla or Space X business models?/0\n",
      "['think', 'amazon', 'adopt', 'house', 'approach', 'manufacture', 'similar', 'tell', 'space', 'X', 'business', 'model']\n",
      "How many baronies might exist within a county palatine?/0\n",
      "['many', 'barony', 'might', 'exist', 'within', 'county', 'palatine']\n",
      "How I know whether a girl had done sex before sex with me?/0\n",
      "['know', 'whether', 'girl', 'sex', 'sex']\n",
      "How do I become a fast learner both in my professional career and in my personal life?/0\n",
      "['become', 'fast', 'learner', 'professional', 'career', 'personal', 'life']\n",
      "Has the United States become the largest dictatorship in the world?/1\n",
      "['united', 'state', 'become', 'large', 'dictatorship', 'world']\n",
      "What is the strangest phenomenon you know of, have witnessed or have generated in the area of electronics that has no explanation in terms of modern physics?/0\n",
      "['strange', 'phenomenon', 'know', 'witness', 'generate', 'area', 'electronics', 'explanation', 'term', 'modern', 'physic']\n",
      "Should I leave my friends and find new ones?/0\n",
      "['leave', 'friend', 'find', 'new', 'one']\n",
      "Can you make Amazon Alexa trigger events in the browser?/0\n",
      "['make', 'amazon', 'alexa', 'trigger', 'event', 'browser']\n",
      "Why haven't two democracies never ever went for a full fledged war? What stops them?/0\n",
      "['two', 'democracy', 'never', 'ever', 'go', 'full', 'fledge', 'war', 'stop']\n",
      "How can I top CBSE in 6 months?/0\n",
      "['top', 'case', 'month']\n",
      "What should I know before visiting Mcleodganj and doing the Triund trek?/0\n",
      "['know', 'visit', 'mcleodganj', 'around', 'trek']\n",
      "How do modern military submarines reduce noise to achieve stealth?/0\n",
      "['modern', 'military', 'submarine', 'reduce', 'noise', 'achieve', 'stealth']\n",
      "Which babies are more sweeter to their parents? Dark skin babies or light skin babies?/1\n",
      "['baby', 'sweet', 'parent', 'dark', 'skin', 'baby', 'light', 'skin', 'baby']\n",
      "How can I remove black heads which are all over my nose?/0\n",
      "['remove', 'black', 'head', 'nose']\n",
      "If lightsabers are created by individual wielders, does each saber have unique powers/abilities?/0\n",
      "['lightsabers', 'create', 'individual', 'fielders', 'saber', 'unique', 'power', 'ability']\n",
      "Is anyone still using Visual Basic? Is it worth learning in 2018? Would there be professional jobs for Visual Basic programmers in 2018-19-20?/0\n",
      "['anyone', 'still', 'use', 'visual', 'basic', 'worth', 'learn', 'would', 'professional', 'job', 'visual', 'basic', 'programmer']\n",
      "What is Sykes Enterprises all about?/0\n",
      "['eyes', 'enterprise']\n",
      "Is there any clear relations between the number of nodes/DOFs and the computational performances and requirements in FEA or CFD analyses (for ANSYS solutions in particular)?/0\n",
      "['clear', 'relation', 'number', 'node', 'does', 'computational', 'performance', 'requirement', 'few', 'cud', 'analysis', 'any', 'solution', 'particular']\n",
      "Why my package still is ISC since May 31,2017 and I don't have updated?/0\n",
      "['package', 'still', 'is', 'since', 'may', 'update']\n",
      "What does great wit mean?/0\n",
      "['great', 'wit', 'mean']\n",
      "In your experience working with Realtors, what do you wish Realtors did better?/0\n",
      "['experience', 'work', 'realtor', 'wish', 'realtor', 'good']\n",
      "How do I get charge by contact?/0\n",
      "['get', 'charge', 'contact']\n",
      "Do all public school teachers automatically get vacation whenever they ask for it?/0\n",
      "['public', 'school', 'teacher', 'automatically', 'get', 'vacation', 'whenever', 'ask']\n",
      "What is the role of technology in using a resource?/0\n",
      "['role', 'technology', 'use', 'resource']\n",
      "Should I opt Jaypee University Guna for mechanical engineering?/0\n",
      "['opt', 'jaypee', 'university', 'guna', 'mechanical', 'engineering']\n",
      "Where can I download Microsoft Word for Windows 2.0 in Hungarian?/0\n",
      "['download', 'microsoft', 'word', 'window', 'hungarian']\n",
      "What do I need to know about buying a car in South Africa as an American?/0\n",
      "['need', 'know', 'buy', 'car', 'south', 'africa', 'american']\n",
      "As someone who didn't enjoy Harry Potter and the Order of the Phoenix movie, can I at least enjoy the book of it?/0\n",
      "['someone', 'enjoy', 'harry', 'potter', 'order', 'phoenix', 'movie', 'least', 'enjoy', 'book']\n",
      "What is the writing style of the book \"How to Resist Prince Charming\" by Linda Kage?/0\n",
      "['write', 'style', 'book', 'resist', 'prince', 'charming', 'linda', 'age']\n",
      "My mother expects me to memorize all her usernames and passwords. How can I make her more responsible about them as I will be going to college in one year?/0\n",
      "['mother', 'expect', 'memorize', 'usernames', 'password', 'make', 'responsible', 'go', 'college', 'one', 'year']\n",
      "What is that movie in which a kid is fooled into thinking that germs will kill him and so he lives in a bubble for most of his life, but then decides to travel the world in a portable germ-free bubble?/0\n",
      "['movie', 'kid', 'fool', 'think', 'germ', 'kill', 'live', 'bubble', 'life', 'decide', 'travel', 'world', 'portable', 'germ', 'free', 'bubble']\n",
      "Why most of the computer science student buy final year project from outside rather doing it by own, is our education system really that week?/0\n",
      "['computer', 'science', 'student', 'buy', 'final', 'year', 'project', 'outside', 'rather', 'education', 'system', 'really', 'week']\n",
      "What are some ways to shorten your period, and what are the risks of doing it?/0\n",
      "['way', 'shorten', 'period', 'risk']\n",
      "Why do we calead leap year.?/0\n",
      "['called', 'leap', 'year']\n",
      "How many days will it take to get rid of spleen enlargement?/0\n",
      "['many', 'day', 'take', 'get', 'rid', 'spleen', 'enlargement']\n",
      "Which machine learning techniques can be used to extract metadata (font color, size, indentation and alignment) of a word document (.docx) file, and can it be integrated with a web application?/0\n",
      "['machine', 'learn', 'technique', 'use', 'extract', 'metadata', 'font', 'color', 'size', 'indentation', 'alignment', 'word', 'document', 'dock', 'file', 'integrate', 'web', 'application']\n",
      "Does it work with girls the way Hitch Will Smith asks not to dance too much?/0\n",
      "['work', 'girl', 'way', 'hitch', 'smith', 'asks', 'dance', 'much']\n",
      "Why India Act 1935 was so special?/0\n",
      "['india', 'act', 'special']\n",
      "Are there any sports that you don't like?/0\n",
      "['sport', 'like']\n",
      "How do DNA and RNA compare and contrast?/0\n",
      "['and', 'ran', 'compare', 'contrast']\n",
      "Someone breaks into your house you shoot and kill them they were armed with only a knife what happens now?/0\n",
      "['someone', 'break', 'house', 'shoot', 'kill', 'arm', 'knife', 'happen']\n",
      "How can I write a biography about Gianni Versace?/0\n",
      "['write', 'biography', 'giant', 'verse']\n",
      "Are extroverted better and faster at processing and expelling information than introverts?/0\n",
      "['introverted', 'well', 'faster', 'process', 'expel', 'information', 'introvert']\n",
      "Have you ever been recognized at a place very far from your home?/0\n",
      "['ever', 'recognize', 'place', 'far', 'home']\n",
      "Why do price comparison websites work well in financial services?/0\n",
      "['price', 'comparison', 'websites', 'work', 'well', 'financial', 'service']\n",
      "Does ragging happen at NIFT Bangalore?/0\n",
      "['rag', 'happen', 'gift', 'bangalore']\n",
      "Why their are so many bad reviews of Bahubali 2 on IMDb?/0\n",
      "['many', 'bad', 'review', 'bahubali', 'mob']\n",
      "Is swallowing Listerine dangerous?/0\n",
      "['swallow', 'listerine', 'dangerous']\n",
      "What are the theories in critical thinking?/0\n",
      "['theory', 'critical', 'thinking']\n",
      "What are the biggest problems, questions, doubts that you come across when trying to choose the paint color for a room?/0\n",
      "['big', 'problem', 'question', 'doubt', 'come', 'across', 'try', 'choose', 'paint', 'color', 'room']\n",
      "How can I get cheap flights in Edinburgh?/0\n",
      "['get', 'cheap', 'flight', 'edinburgh']\n",
      "What is China's new chick?/0\n",
      "[\"china's\", 'new', 'chick']\n",
      "How do I send large picture files through an email?/0\n",
      "['send', 'large', 'picture', 'file', 'email']\n",
      "Why doesn't eBay allow the sale of WWII purple heart medals even though they have categories specifically for WWII military medals?/0\n",
      "['bay', 'allow', 'sale', 'wait', 'purple', 'heart', 'medal', 'even', 'though', 'category', 'specifically', 'wait', 'military', 'medal']\n",
      "What are the characteristics that define isovolumetric relaxation?/0\n",
      "['characteristic', 'define', 'isovolumetric', 'relaxation']\n",
      "How do I work in cybersecurity overseas?/0\n",
      "['work', 'cybersecurity', 'overseas']\n",
      "Do web developer refer to W3C standard practice?/0\n",
      "['web', 'developer', 'refer', 'W three c', 'standard', 'practice']\n",
      "Why has the internet become a problem? Why do we rely on it for everything?/0\n",
      "['internet', 'become', 'problem', 'rely', 'everything']\n",
      "Can we get ITC on charges levied by banks?/0\n",
      "['get', 'it', 'charge', 'levy', 'bank']\n",
      "Where is Muhammed now?/0\n",
      "['muhammed']\n",
      "What are the services that can be provided by a food testing lab and what are their certified requirements?/0\n",
      "['service', 'provide', 'food', 'test', 'lab', 'certified', 'requirement']\n",
      "Is it ok to be solo your whole life and pretend no one exists? Is there such thing as being unwanted?/0\n",
      "['ok', 'solo', 'whole', 'life', 'pretend', 'one', 'exist', 'thing', 'unwanted']\n",
      "How do I change the owner of a current YouTube account? The original Owner was let go and I do not have access to change anything./0\n",
      "['change', 'owner', 'current', 'youtube', 'account', 'original', 'owner', 'let', 'go', 'access', 'change', 'anything']\n",
      "Are the Archer characters animated based on celebrities?/0\n",
      "['archer', 'character', 'animate', 'base', 'celebrity']\n",
      "What is the best way to propose a girl without annoying her?/0\n",
      "['best', 'way', 'propose', 'girl', 'without', 'annoy']\n",
      "What can $500 million get you in solar power?/0\n",
      "['million', 'get', 'solar', 'power']\n",
      "What are the recommended 2D game engines for a beginning Python programmer?/0\n",
      "['recommended', 'two D', 'game', 'engine', 'beginning', 'python', 'programmer']\n",
      "Is there such a thing as teleological pantheism?/0\n",
      "['thing', 'teleological', 'pantheism']\n",
      "What are some best college for aircraft propulsion(M.S)?/0\n",
      "['best', 'college', 'aircraft', 'propulsion']\n",
      "What countries in the world have freedom of speech on par with the US?/0\n",
      "['country', 'world', 'freedom', 'speech', 'par', 'U']\n",
      "What lead to the Red Terror in Ethiopia?/0\n",
      "['lead', 'red', 'terror', 'ethiopia']\n",
      "What is the best pen at low cost in all sorts of things?/0\n",
      "['best', 'pen', 'low', 'cost', 'sort', 'thing']\n",
      "Do you trust a business that has a Facebook page as its website?/0\n",
      "['trust', 'business', 'facebook', 'page', 'website']\n",
      "Should we improve our piano skill by keep practicing hard pieces? Is that the most effective way?/0\n",
      "['improve', 'piano', 'skill', 'keep', 'practice', 'hard', 'piece', 'effective', 'way']\n",
      "I wear an insulin pump, and a lot of girls don't like it. Nick Jonas wears one and dated Selena Gomez. Is there difference between me and Nick several zeros missing in my bank account?/0\n",
      "['wear', 'insulin', 'pump', 'lot', 'girl', 'like', 'nick', 'jonas', 'wear', 'one', 'date', 'selena', 'gomez', 'difference', 'nick', 'several', 'zero', 'miss', 'bank', 'account']\n",
      "Will Turkey be seperated and give a land to Kurds because of foreign powers like USA in the future?/0\n",
      "['turkey', 'separated', 'give', 'land', 'kurds', 'foreign', 'power', 'like', 'us', 'future']\n",
      "R sq. cos-1 when r=18.5 equals?/0\n",
      "['R', 'S Q', 'C O', 'R', 'equal']\n",
      "Who sings the song in my head?/0\n",
      "['sing', 'song', 'head']\n",
      "Can Chronicled replace supply Chain?/0\n",
      "['chronicle', 'replace', 'supply', 'chain']\n",
      "How can you solve this equationâ€¦ Sin(a*t)-b*t=0?/0\n",
      "['solve', '', 'sin', 'B']\n",
      "What are the demerits of excellence in academic pursuits?/0\n",
      "['demerit', 'excellence', 'academic', 'pursuit']\n",
      "How many Indians are in Melbourne, Australia?/0\n",
      "['many', 'indian', 'melbourne', 'australia']\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for text in tqdm(training_dataset['question_text'].iloc[:100], disable=('verbose')):\n",
    "    print(text + \"/\" + str(training_dataset['target'].iloc[index]))\n",
    "    print(preprocess_text(text))\n",
    "    index = index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ryan', \"'s\", 'book']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_sent(\"Ryan's book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ryan', \"'s\", 'book']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Ryan's book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'old', 'english', 'pure', 'english']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence('is old english \"pure english\"?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING NSW DICTIONARY\n",
      "-----------------------\n",
      "\n",
      "0 NSWs found\n",
      "\n",
      "TAGGING NSWs\n",
      "------------\n",
      "\n",
      "0 of 0 tagged\n",
      "\n",
      "SPLITTING NSWs\n",
      "--------------\n",
      "\n",
      "0 of 0 split\n",
      "\n",
      "RETAGGING SPLIT NSWs\n",
      "--------------------\n",
      "\n",
      "0 of 0 retagged\n",
      "\n",
      "CLASSIFYING ALPHABETIC NSWs\n",
      "---------------------------\n",
      "\n",
      "0 of 0 classified\n",
      "\n",
      "CLASSIFYING NUMERIC NSWs\n",
      "------------------------\n",
      "\n",
      "0 of 0 classified\n",
      "\n",
      "CLASSIFYING MISCELLANEOUS NSWs\n",
      "------------------------------\n",
      "\n",
      "0 of 0 classified\n",
      "\n",
      "EXPANDING ALPHABETIC NSWs\n",
      "-------------------------\n",
      "\n",
      "0 of 0 expanded\n",
      "\n",
      "EXPANDING NUMERIC NSWs\n",
      "----------------------\n",
      "\n",
      "0 of 0 expanded\n",
      "\n",
      "EXPANDING MISCELLANEOUS NSWs\n",
      "----------------------------\n",
      "\n",
      "0 of 0 expanded\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"who's\", 'your', 'daddy']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise.normalise(text_to_word_sequence(\"Who's your daddy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Ryan's\", 'book']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise.normalise(\"Ryan's book\", user_abbrevs=custom_dictionary, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reaction', 'nahso4+nacl=na2so4+hcl']\n"
     ]
    }
   ],
   "source": [
    "texts = ['reaction', 'nahso4+nacl=na2so4+hcl']\n",
    "\n",
    "try: \n",
    "    normalise.normalise(texts, verbose=False)\n",
    "except:\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            result.append(normalise.normalise(texts, verbose=False))\n",
    "        except:\n",
    "            result.append(text)\n",
    "print(result)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING NSW DICTIONARY\n",
      "-----------------------\n",
      "\n",
      "3 NSWs found\n",
      "\n",
      "TAGGING NSWs\n",
      "------------\n",
      "\n",
      "3 of 3 tagged\n",
      "\n",
      "SPLITTING NSWs\n",
      "--------------\n",
      "\n",
      "1 of 1 split\n",
      "\n",
      "RETAGGING SPLIT NSWs\n",
      "--------------------\n",
      "\n",
      "2 of 2 retagged\n",
      "\n",
      "CLASSIFYING ALPHABETIC NSWs\n",
      "---------------------------\n",
      "\n",
      "2 of 2 classified\n",
      "\n",
      "CLASSIFYING NUMERIC NSWs\n",
      "------------------------\n",
      "\n",
      "2 of 2 classified\n",
      "\n",
      "CLASSIFYING MISCELLANEOUS NSWs\n",
      "------------------------------\n",
      "\n",
      "0 of 0 classified\n",
      "\n",
      "EXPANDING ALPHABETIC NSWs\n",
      "-------------------------\n",
      "\n",
      "2 of 2 expanded\n",
      "\n",
      "EXPANDING NUMERIC NSWs\n",
      "----------------------\n",
      "\n",
      "2 of 2 expanded\n",
      "\n",
      "EXPANDING MISCELLANEOUS NSWs\n",
      "----------------------------\n",
      "\n",
      "0 of 0 expanded\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['four form',\n",
       " 'house',\n",
       " 'for',\n",
       " 'sale',\n",
       " ',',\n",
       " 'four hundred and fifty nine thousand pounds',\n",
       " 'ONO']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_abbreviations = {\"bdrm\": \"bedroom\",\n",
    "                    \"KT\": \"kitchen\",\n",
    "                    \"wndw\": \"window\",\n",
    "                    \"ONO\": \"or near offer\"}\n",
    "\n",
    "text = [\"4bdrm\", \"house\", \"for\", \"sale\", \",\", \"Â£459k\", \"ONO\"]\n",
    "\n",
    "normalise.normalise(text, user_abbrevs={\"ONO\": \"or near offer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lemmatize_sent(\"how are dangerous are spaag vehicles such as the zsu-23-4 against helicopters and drones?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'be', 'dangerous', 'be', 'spaag', 'vehicle', 'such', 'a', 'the', 'zsu', '23', '4', 'against', 'helicopter', 'and', 'drone']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\derekhsu\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'WordListCorpusReader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-4a5780261ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word)\n",
    "    print(word in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
